%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% Preamble %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Word target: 2,500 words

% Outline:
% I. Introduction (400 words) (~379 currently)
% II. GCE estimator (450 words) (~445 currently)
% III. Nested GCE estimator (350 words) (~335 currently)
% IV. Sampling Experiments (450 words) (~443)
% V. Results (600 words)
% VI. Conclusions (250 words)

% Declare document class and miscellaneous packages
\documentclass[english]{article}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{ctable}
\usepackage{setspace}
\usepackage{longtable}
\usepackage{url}
\usepackage{moredefs,lips} 
\usepackage{IEEEtrantools}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage[normalsize]{caption}
\usepackage{subcaption}
\usepackage{afterpage}
\usepackage[all]{nowidow}
\usepackage{listings}
%\usepackage{fullpage}
\urlstyle{rm}

%Hyper-references
\usepackage{hyperref}
\hypersetup{colorlinks, citecolor=black, filecolor=black, linkcolor=black, 
urlcolor=black, pdftex}

% Title page
\title{Improving Small Sample Performance of the Generalized Cross 
Entropy Estimator}
\author{
Heath Henderson\thanks{Corresponding author; 1300 New York Avenue 
NW, Washington, DC 20577; Tel: +1 202 623 3860; Fax: +1 202 312 4202; 
Email: heathh@iadb.org.}\\
\textit{Office of Strategic Planning and Development Effectiveness} \\
\textit{Inter-American Development Bank} \\
\\
Amos Golan \\
\textit{Department of Economics}\\
\textit{American University} \\
\\
Skipper Seabold \\
\textit{Department of Economics}\\
\textit{American University}
\\ \\
}

\date{\today}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% Title Page and Abstract %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{titlepage}
\maketitle

%\begin{abstract}
%In the estimation of linear and non-linear models, the generalized maximum 
%entropy (GME) estimator has been found to outperform its traditional 
%counterparts.
%The generalized cross entropy (GCE) estimator is a generalization of GME
%that permits the incorporation of prior information.
%While informative priors can improve the performance of the GCE estimator 
%relative to GME, prior information is often missing and achieving improved 
%performance implies choosing among a possibly infinite number of priors.
%Through extensive sampling experiments, we thus identify a general basis 
%for selecting among alternative prior distributions in an effort to improve the 
%performance of the GCE estimator. \\
%~\\
%\textit{Key words}: Generalized maximum entropy; Generalized cross 
%entropy; Prior information\\
%~ \\
%\textit{JEL codes}: C13; C14  
%\end{abstract}
\thispagestyle{empty}
\end{titlepage}
\newpage

% Set citations without comma between author and year
\bibpunct{(}{)}{;}{a}{}{,}

\doublespacing

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% Introduction %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec: intro}

The generalized maximum entropy (GME) estimator seeks to recover the 
unknown probability distributions that characterize a given dataset by 
maximizing the Shannon (\citeyear{shannon1948}) entropy or information 
measure subject to stochastic moment conditions and proper probability 
constraints.%
\footnote{The estimator is termed ``generalized'' because it is a 
generalization of classical maximum entropy (ME) formalism, which treats
moment conditions as non-stochastic \citep{jaynes1957a, jaynes1957b}.}
In the estimation of both linear and non-linear models, the GME estimator
has been found to outperform its traditional counterparts (e.g.\ ordinary 
least squares and maximum likelihood estimators), especially in the context 
of ill-posed, ill-conditioned, or noisy problems \citep{golan1996}.
As such, the GME estimator has witnessed widespread application in the 
social and natural sciences.

The generalized cross entropy (GCE) estimator instead uses the 
Kullback-Leibler (\citeyear{kullback1951}) divergence metric to minimize the 
discrepancy between the posterior probabilities and the researcher's chosen 
priors.
For uniform priors, it can be shown that GME and GCE formalism yield
identical solutions, and thus GCE is a generalization of the GME estimator
\citep{golan1996}.
While more informative reference distributions can improve the performance 
of the GCE estimator relative to GME,%
\footnote{For example, see \citet{heckelei2003}. 
It is also important here to clarify our use of the term ``informative.''
Shannon entropy measures the expected informational content or 
uncertainty associated with a random variable.
The measure reaches a maximum for uniformly distributed random 
variables, which corresponds to a state of complete ignorance.
The measure equals zero when one of the probabilities in the probability 
mass function is exactly one, which corresponds to a state of perfect 
certainty.
It is in this sense that non-uniform priors are more informative.
See \citet{golan2008} for further discussion. \label{fn: info}}
such prior information is often missing and achieving improved performance 
implies choosing among a possibly infinite number of prior probability 
distributions.
We thus identify a general basis for selecting among alternative reference 
distributions and develop a nested GCE estimator that improves 
upon the small sample performance of the GCE estimator. 

In what follows, Section \ref{sec: gce} discusses GCE formalism and 
Section \ref{sec: ngce} outlines our nested GCE estimator.
Section \ref{sec: mce} details the sampling experiments through which we
compare the performance of our estimation strategy to the leading 
alternative estimators.
Section \ref{sec: results} elaborates upon the results of the 
experiments and Section \ref{sec: conc} concludes. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%% The GCE Estimator  %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The GCE Estimator}
\label{sec: gce}

Following \cite{golan1996} and \citet{golan2008}, consider the following 
linear regression model: 
\begin{equation}
\mathbf{y} = \mathbf{X\beta} + \mathbf{\varepsilon}
\end{equation}

\noindent
where $\mathbf{y}$ is a $T$-dimensional vector of observations on the 
dependent variable, $\mathbf{X}$ is a $T\times K$ matrix of exogenous
variables, $\mathbf{\beta}$ is a $K$-dimensional vector of unknown 
parameters, and $\mathbf{\varepsilon}$ is a $T$-dimensional vector of 
random errors.
Each $\mathbf{\beta}_k$ and $\mathbf{\varepsilon}_t$ in the GCE 
framework is typically viewed as the mean value of some well-defined 
random variable, which we denote as $\mathbf{z}_k$ and $\mathbf{v}_t$, 
respectively.
Accordingly, let $\mathbf{p}_k$ be an $M$-dimensional proper 
probability distribution defined on the support $\mathbf{z}_k$ such that 
$\mathbf{\beta}_k = \sum_m p_{km}z_{km} = \mathbf{z}_k' 
\mathbf{p}_k$.
Similarly, let $\mathbf{w}_t$ be a $J$-dimensional proper probability 
distribution defined on the support $\mathbf{v}_t$ such that  
$\mathbf{\varepsilon}_t = \sum_j w_{tj}z_{tj} = \mathbf{v}_t' 
\mathbf{w}_t$.

Without loss of generality, the linear regression model can then be 
reparameterized as follows:
\begin{equation}
\mathbf{y} = \mathbf{X\beta} + \mathbf{\varepsilon} = 
\mathbf{X Z p} + \mathbf{V w}
\label{eq: reparm}
\end{equation}

\noindent
where, letting $\mathbf{z}=[\mathbf{z}_1' ~ \mathbf{z}_2' ~ \cdots ~ 
\mathbf{z}_K' ]'$ and $\mathbf{v}=[\mathbf{v}_1' ~ \mathbf{v}_2' ~ 
\cdots ~ \mathbf{v}_T' ]'$, 
$\mathbf{Z}= (\mathbf{I}_K \otimes \mathbf{1}_M')\mathbf{z}$ and
$\mathbf{V}= (\mathbf{I}_T \otimes \mathbf{1}_J')\mathbf{v}$
where $\mathbf{1}$ denotes a vector of ones, $\mathbf{I}$ is the 
identity matrix, and $\otimes$ is the Kronecker product.
Further, $\mathbf{p} = [\mathbf{p}_1' ~ \mathbf{p}_2' ~ \cdots ~ 
\mathbf{p}_K' ]'$ and $\mathbf{w} = [\mathbf{w}_1' ~ \mathbf{w}_2' 
~ \cdots ~ \mathbf{w}_T' ]'$.
The dimensions of $\mathbf{Z}$ and $\mathbf{V}$ are then 
$K \times KM$ and $T \times TJ$, respectively, and the dimensions of 
$\mathbf{p}$ and $\mathbf{w}$ are $KM \times 1$ and $TJ \times 1$, 
respectively.
It should be noted that while it is possible to construct unbounded and 
continuous supports,%
\footnote{For example, see \citet{golan2002}.}
for the sake of simplicity the above support spaces are constructed as 
discrete and bounded. 

Let $\mathbf{q}$ be a $KM$-dimensional vector of prior weights for the 
parameters $\mathbf{\beta}$ with prior mean $\mathbf{Zq}$.
Analogously, let $\mathbf{u}$ be a $TJ$-dimensional vector of prior weights 
for the disturbances $\mathbf{\varepsilon}$ with prior mean $\mathbf{Vu}$.
The GCE estimator then selects $\mathbf{p}$, $\mathbf{w}$ $\gg$ 
$\mathbf{0}$ to minimize 
\begin{equation}
I({\mathbf{p}, \mathbf{q}, \mathbf{w}, \mathbf{u}}) = 
\mathbf{p}' \ln (\mathbf{p}/\mathbf{q}) + 
\mathbf{w}' \ln (\mathbf{w}/\mathbf{u})
\label{eq: ce}
\end{equation}

\noindent
subject to
\begin{equation}
\mathbf{y} = \mathbf{X Z p} 
+ \mathbf{V w}
\label{eq: glm}
\end{equation}
\begin{equation}
\mathbf{1}_K = (\mathbf{I}_K \otimes \mathbf{1}_M')\mathbf{p}
\label{eq: pp1}
\end{equation}
\begin{equation}
\mathbf{1}_T = (\mathbf{I}_T \otimes \mathbf{1}_J')\mathbf{w}
\label{eq: pp2}
\end{equation}

\noindent
where Eq.\ (\ref{eq: glm}) is the data constraint and Eqs.\ 
(\ref{eq: pp1})-(\ref{eq: pp2}) are proper probability constraints.
The reader is referred to \citet[Chap.\ 6]{golan1996} or 
\citet[Chap.\ 6]{golan2008} for analytical solutions, discussion of 
efficient techniques for computation of the GCE solutions via the 
unconstrained dual version of the problem, and issues of inference. 

Intuitively, the values of $\mathbf{p}$ and $\mathbf{w}$ that minimize
$I(\cdot)$ are those that, out of all probabilities satisfying the constraints, 
are ``closest'' to the researcher's chosen priors.
That is, $I(\cdot)$ is a measure of the discrepancy between two probability 
distributions and, for given prior probabilities, the objective is to find the 
posterior probabilities that minimize this discrepancy function.%
\footnote{See \citet[Chap.\ 6]{judge2011} for a full discussion of the 
cross entropy (or Kullback-Leibler) objective function.}
It can further be shown that GCE is a generalization of GME formalism.
That is, with uniform priors the GCE solution is identical to that which 
maximizes $H(\mathbf{p},\mathbf{w}) = - \mathbf{p}' \ln 
(\mathbf{p}) - \mathbf{w}' \ln (\mathbf{w})$ subject to the data
consistency and proper probability constraints. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% The Nested GCE Estimator  %%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Nested GCE Estimator}
\label{sec: ngce}

A central question in GCE estimation pertains to the choice of 
priors or reference distributions.
Prior information is frequently incomplete or simply missing, and 
the researcher must choose among a possibly infinite number of 
reference distributions. 
While in such situations priors are often specified as uniform (i.e.\ 
$\mathbf{q}=1/M$ and $\mathbf{u}=1/J$), more informative 
reference distributions can enhance small sample performance, especially 
for ill-posed, ill-conditioned, or noisy problems.%
\footnote{See footnote \ref{fn: info}.}
Accordingly, we identify a general basis for selecting among alternative 
reference distributions and develop a nested GCE estimator that improves 
upon the small sample performance of the GCE estimator. 

Consider the choice of prior in the context of GCE estimation of the simple 
linear model as outlined above.
Our nested GCE estimator simply minimizes Eq.\ (\ref{eq: ce}) subject to 
Eqs.\ (\ref{eq: glm})-(\ref{eq: pp2}) for (potentially many) alternative prior 
choices and then selects the model for which $I(\cdot)$ is itself minimized.
In other words, among the alternative models, we select the model for which the 
prior and posterior probabilities are ``closest.''
While computational burden places limits on the quantity of alternative 
reference distributions incorporated, the below sampling experiments show 
that a relatively small number of priors is sufficient for the nested GCE 
estimator to outperform its traditional counterparts.

The question then remains as to which alternative reference distributions to 
incorporate.
Noting that $\mathbf{q}=[\mathbf{q}_1' ~ \mathbf{q}_2' ~ 
\cdots ~ \mathbf{q}_K' ]'$, for the sake of exposition focus here on the 
choice of $\mathbf{q}_2$ and let $\mathbf{q}_k$ be uniform for 
$k=1,3,4,\ldots, K$.%
\footnote{Each $\mathbf{q}_k$ here is an $M$-dimensional vector of 
 prior weights for the parameter $\beta_k$.}
In developing candidate priors, we define proper probabilities on the vector 
$\mathbf{z}_2$ such that across alternative choices of $\mathbf{q}_2$ the 
expectation $\mathbf{z}_2'\mathbf{q}_2$  yields values that appropriately
span the support space. 
Let $S$ represent all  $M!$ possible permutations of 
$s = [1^\rho ~ 2^\rho ~ \cdots ~ M^\rho]$.
Normalizing each element of $S$ by $\sum_{m=1}^M m^\rho$, it is 
readily verified that we have $M!$ candidate $M$-dimensional vectors for 
$\mathbf{q}_2$ that span the support space.%
\footnote{The parameter $\rho \in [-\infty, \infty]$ permits the researcher
to control the dispersion of the alternative prior means about the center of the 
support space.
Notably, as $\rho \to 0$ all priors approach uniform and as $|\rho| \to 
\infty$ the prior means approach the boundaries of the support space.}
Also incorporating the uniform prior $[1/M ~ 1/M ~ \cdots ~ 1/M]$ yields 
$M! + 1$ priors.
The nested GCE estimator in this case minimizes Eq.\ (\ref{eq: ce}) subject 
to Eqs.\ (\ref{eq: glm})-(\ref{eq: pp2}) after inserting into $\mathbf{q}$ 
each of the $M! + 1$ priors.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% Sampling Experiments %%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Sampling Experiments}
\label{sec: mce}

For each replication in the sampling experiments our objective is to minimize 
$I(\cdot)$ subject to the data consistency and proper probability constraints 
for alternative choices of $\mathbf{q}$, the prior weights for the parameter 
$\mathbf{\beta}$. 
To measure the performance of each prior we focus on the mean squared 
error $MSE(\hat{\mathbf{\beta}}) = E[(\hat{\mathbf{\beta}}- 
\mathbf{\beta})^2]$.
To establish a basis for selecting among alternative priors, the relationship 
between $MSE(\hat{\mathbf{\beta}})$ and $E[I(\cdot)]$ is examined.
This relationship is considered for different sample sizes, levels of noise, 
degrees of correlation among covariates, and parameter support vectors.
Further, the performance of our nested GCE estimator is referenced 
against the OLS and GME estimators.

Consider a baseline scenario where, referencing Eq.\ (\ref{eq: reparm}),
$\mathbf{X}$ is a $10 \times 3$ design matrix where $\mathbf{x}_k \sim 
U(0,20)$, $\mathbf{\beta}= [\beta_1 ~ \beta_2 ~ \beta_3]'=[1 ~ -5 ~ 2]'$, 
and $\mathbf{\varepsilon} \sim N(0,2)$.
In order to control correlation among covariates, the condition
number $\kappa(\mathbf{X}'\mathbf{X})=1$ is specified \textit{a priori}
by appropriately replacing the eigenvalues in the singular value 
decomposition of $\mathbf{X}$.%
\footnote{See \citet[pg.\ 133]{golan1996} and references therein for 
details.}
Further, letting $M=J=3$, the support vector on $\mathbf{p}_k$ is 
$\mathbf{z}_k = [- 10 ~ 0 ~ 10]'$ and the support vector on $\mathbf{w}_t$ 
is $\mathbf{v}_t = [- 3\sigma_y ~ 0 ~ 3\sigma_y]'$.%
\footnote{The error support vector is based on the three-sigma
rule \citep{pukelsheim1994} and is calculated 
uniquely for each replication based on the observed $\mathbf{y}$.}
Regarding prior information, let $\mathbf{q} = [\mathbf{q}_1' ~ 
\mathbf{q}_2' ~ \mathbf{q}_3']'$ represent prior information on the 
respective elements of the parameter vector.
As in Section \ref{sec: ngce}, we let $\mathbf{q}_1 = \mathbf{q}_3 = 1/M 
= 1/3$ (i.e.\ uniform) and estimate the model using $\rho=2$  in the 
discussed alternative specifications of $\mathbf{q}_2$.% 
\footnote{That is, the focus here is on examining alternative reference 
distributions for the parameter $\beta_2$. 
However, with respect to the performance of the estimator, we are 
interested in the MSE across all parameters.
As will be seen, with correlation among covariates the specification of the 
prior on $\beta_2$ affects the estimation of other parameters.}
Throughout we also use uniform priors on the error term 
(i.e.\ $\mathbf{u} = 1/J = 1/3$).
Finally, the experiment is conducted with $N=1,000$ replications.

As mentioned, we also conduct experiments with different sample sizes, 
levels of noise, degrees of correlation among covariates, and parameter 
support vectors.
Regarding sample sizes, beyond $T=10$ we consider $T=20, 50, 100, 
\text{and } 500$. 
With respect to noise, we increase the level of noise in the experiments and 
let $\mathbf{\varepsilon}\sim N(0,5)$.
Further, while in the baseline $\kappa(\mathbf{X}'\mathbf{X})=1$, we
also let $\kappa(\mathbf{X}'\mathbf{X})=90$, which represents a 
moderately ill-conditioned design matrix.
Finally, we examine the sensitivity of the results to the parameter support
vector by additionally letting $\mathbf{z}_k = [- 100 ~ 0 ~ 100]'$.
As summarized in Table \ref{tbl: exp}, there are then a total of 20 
experimental specifications. 
That is, the experiments are conducted with five alternative sample sizes, 
where for each sample size we consider a baseline, well-behaved case along
with noisy, collinear, and altered-support cases.%
\footnote{The sampling experiments were implemented in Python 
2.7.3. 
We also conducted experiments that used different parameter vectors, 
increased the number of covariates, and utilized the prior procedure on
multiple parameters.
Additional results and computer code are available upon request.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% Results %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results}
\label{sec: results}

% Baseline case
%	- Present graph for well-behaved case and discuss MSE estimates
%          (Maybe look at CE signal vs. MSE too here)
%	- Present graph for noisy case and discuss MSE estimates
%	- Present graph for correlated case and discuss MSE estimates
%	- Present graph for altered-support case and discuss MSE estimates

%\begin{figure}
%\centering
%\begin{subfigure}{.5\textwidth}
%  \centering
%  \includegraphics[width=1\linewidth]{01020101figure(dev_ent)(ce_signal)(means).pdf}
%  \caption{A subfigure}
%  \label{fig:sub1}
%\end{subfigure}%
%\begin{subfigure}{.5\textwidth}
%  \centering
%  \includegraphics[width=1\linewidth]{01020101figure(dev_ent1)(ce_total)(means).pdf}
%  \caption{A subfigure}
%  \label{fig:sub2}
%\end{subfigure}
%\caption{A figure with two subfigures}
%\label{fig:test}
%\end{figure}
%
%\begin{figure}
%\centering
%\includegraphics[width=0.75\linewidth]{01020101figure(dev_ent)(ce_signal)(means).pdf}
% \caption{A subfigure}
%\label{fig:test}
%\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% Conclusions %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusions}
\label{sec: conc}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% Appendix %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section*{Appendix}
\label{sec: appendix}

% Sampling Experiments
\footnotesize
\ctable[
cap = {Sampling Experiments},
caption = {Sampling Experiments},
captionskip = -2ex,
pos=htb,
label = {tbl: exp}
]{lcccc}{
}{\hline \hline
Experiment & \multicolumn{1}{c}{$T$} & 
\multicolumn{1}{c}{$\varepsilon$} &
\multicolumn{1}{c}{$\kappa(\mathbf{X}'\mathbf{X})$} &
\multicolumn{1}{c}{$\mathbf{z}_k$} \\ \hline
 1 & 10 & $N(0,2)$ &   1  & $[-10 ~ 0 ~ 10]'$  \\ 
 2 & 20 & $N(0,2)$ &   1  & $[-10 ~ 0 ~ 10]'$  \\ 
 3 & 50 & $N(0,2)$ &   1  & $[-10 ~ 0 ~ 10]'$  \\ 
 4 & 100 & $N(0,2)$ &   1  & $[-10 ~ 0 ~ 10]'$  \\ 
 5 & 500 & $N(0,2)$ &   1  & $[-10 ~ 0 ~ 10]'$  \\ 
6 & 10 & $N(0,5)$ &   1  & $[-10 ~ 0 ~ 10]'$ \\ 
7 & 20 & $N(0,5)$ &   1  & $[-10 ~ 0 ~ 10]'$ \\
8 & 50 & $N(0,5)$ &   1  & $[-10 ~ 0 ~ 10]'$ \\  
9 & 100 & $N(0,5)$ &   1  & $[-10 ~ 0 ~ 10]'$ \\
10 & 500 & $N(0,5)$ &   1  & $[-10 ~ 0 ~ 10]'$ \\  
11 & 10 & $N(0,2)$ & 90  & $[-10 ~ 0 ~ 10]'$ \\
12 & 20 & $N(0,2)$ & 90  & $[-10 ~ 0 ~ 10]'$ \\ 
13 & 50 & $N(0,2)$ & 90  & $[-10 ~ 0 ~ 10]'$ \\ 
14 & 100 & $N(0,2)$ & 90  & $[-10 ~ 0 ~ 10]'$ \\ 
15 & 500 & $N(0,2)$ & 90  & $[-10 ~ 0 ~ 10]'$ \\    
16 & 10 & $N(0,2)$ &   1  & $[-100 ~ 0 ~ 100]'$ \\ 
17 & 20 & $N(0,2)$ &   1  & $[-100 ~ 0 ~ 100]'$ \\ 
18 & 50 & $N(0,2)$ &   1  & $[-100 ~ 0 ~ 100]'$ \\ 
19 & 100 & $N(0,2)$ &   1  & $[-100 ~ 0 ~ 100]'$ \\
20 & 500 & $N(0,2)$ &   1  & $[-100 ~ 0 ~ 100]'$ \\ \hline}
\normalsize

% Mean squared error (B1)
\footnotesize
\ctable[
cap = {Mean Squared Error $\hat{\beta}_2$},
caption = {Mean Squared Error $\hat{\beta}_2$},
captionskip = -2ex,
pos=htb,
label = {tbl: mseb1}
]{lccc}{
}{\hline \hline
Experiment & \multicolumn{1}{c}{NGCE} & 
\multicolumn{1}{c}{GME} &
\multicolumn{1}{c}{OLS}  \\ \hline
 1 &  \\ 
 2 & \\ 
 3 &  \\ 
 4 &   \\ 
 5 &   \\ 
6 & \\ 
7 & \\
8 & \\  
9 &  \\
10 &  \\  
11 & \\
12 & \\ 
13 & \\ 
14 &  \\ 
15 & \\    
16 &  \\ 
17 &  \\ 
18 &  \\ 
19 & \\
20 & \\ \hline}
\normalsize

% Mean squared error (B)
\footnotesize
\ctable[
cap = {Mean Squared Error $\hat{\beta}$},
caption = {Mean Squared Error $\hat{\beta}$},
captionskip = -2ex,
pos=htb,
label = {tbl: mseb1}
]{lccc}{
}{\hline \hline
Experiment & \multicolumn{1}{c}{NGCE} & 
\multicolumn{1}{c}{GME} &
\multicolumn{1}{c}{OLS}  \\ \hline
 1 &  \\ 
 2 & \\ 
 3 &  \\ 
 4 &   \\ 
 5 &   \\ 
6 & \\ 
7 & \\
8 & \\  
9 &  \\
10 &  \\  
11 & \\
12 & \\ 
13 & \\ 
14 &  \\ 
15 & \\    
16 &  \\ 
17 &  \\ 
18 &  \\ 
19 & \\
20 & \\ \hline}
\normalsize

%In line with Section \ref{sec: ngce}, for $M=3$, $S$ represents the $3! = 3 
%\times 2 \times 1 = 6$ permutations of $s = [1, 2, 3]$. 
%Normalizing each element by $\sum_{m=1}^{3} m = 6$, we then have all 
%permutations of $s = [1/6, 2/6, 3/6] = [0.167, 0.333, 0.500]$.
%To be explicit, in $S$ we then have
%\begin{align*}
%[0.167, 0.333, 0.500] \\
%[0.167, 0.500, 0.333] \\
%[0.333, 0.167, 0.500] \\
%[0.333, 0.500, 0.167] \\
%[0.500, 0.167, 0.333] \\
%[0.500, 0.333, 0.167] 
%\end{align*}
%
%\noindent
%To this we have chosen to add
%\begin{align*}
%[0.650, 0.300, 0.050] \\
%[0.050, 0.300, 0.650] \\
%[0.333, 0.333, 0.333]
%\end{align*}
%
%\noindent
%The former two priors in the above reach the more extreme parts of the 
%support space and the latter prior is simply the uniform.
%For each replication in the sampling experiments, $I(\cdot)$ is then 
%minimized (subject to the relevant constraints) a total of nine times where
%each time $\mathbf{q}_2$ take on a different element in $S$. 
%While it is clearly feasible to incorporate additional priors, the above listing 
%appears sufficient for illustrating our estimation technique.
%Finally, it should be noted that the uniform prior is in the above 
%and, as such, the GME estimator is included as a special case in the 
%experiments. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% References %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Start fresh page
\newpage
\cleardoublepage
\singlespacing

%Declare the style of the bibliography
\bibliographystyle{au-cms}

%Specify the file to use
\bibliography{/Users/hendersonhl/Documents/References}
\newpage

\end{document}